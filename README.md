# Lerobot Controlled by hand

[![License](https://img.shields.io/github/license/ABMI-software/hand_controlLerobot)](LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.6%2B-blue)](https://www.python.org/downloads/)

## Overview
A robot arm teleoperation system based on hand tracking via webcam. Lerobot Control enables intuitive control of the SO-101 robot through natural hand gestures, bridging the gap between human motion and robotic control with a seamless real-time interface.

### Key Benefits
- ðŸŽ¯ Intuitive Control: Natural hand movements translate directly to robot actions
- âš¡ Real-time Response: Minimal latency between gesture recognition and robot movement
- ðŸ”„ Flexible Tracking: Multiple tracking models available for different use cases
- ðŸ›  Customizable: Adjustable sensitivity and control parameters

### What's New (Nov 17, 2025)
- **Headless & CI-friendly:** The repo now supports running on headless systems (WSL/CI) â€” see `scripts/run_poke_motor.sh` and the new `TROUBLESHOOTING.md` for env hints.
- **Clear camera handling:** Camera input accepts integer indices or file/device paths; code now fails with a clear, actionable message when no camera is present, and suggests using a video file for testing.
- **Lazy kinematics import:** The `pinocchio` dependency is loaded lazily â€” the app no longer fails at startup if `pinocchio` is missing (only required when using URDF/IK features).
- **Improved error messages & docs:** Helpful guidance added for common issues and step-by-step troubleshooting in `TROUBLESHOOTING.md`.
- **Unit tests:** New utility tests added for core mapping/clamping routines (`tests/test_utils.py`).


## Table of Contents
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)

## Features
- Real-time hand tracking and gesture recognition
- Control of robot joints and gripper
- Support for multiple tracking models (Wilor, MediaPipe)
- Object detection using YOLOv8 with Arducam
- Dual camera system (Arducam + webcam)
- Interactive mode selection (auto-pick/teleoperation)
- Adjustable parameters for sensitivity and control
- Safety limits and emergency stops
- Gesture recording and playback capabilities

## Project Structure
```
# Repository layout (top-level)
.
â”œâ”€â”€ assets/                         # Images, models and other assets
â”œâ”€â”€ hand_teleop/                    # Core teleoperation package
â”‚   â”œâ”€â”€ cameras/                    # Camera handling and CameraManager
â”‚   â”‚   â””â”€â”€ camera_manager.py
â”‚   â”œâ”€â”€ detection/                  # Object detection (YOLOv8 wrapper)
â”‚   â”‚   â””â”€â”€ object_detector.py
â”‚   â”œâ”€â”€ gripper_pose/               # Gripper pose computation + utils
â”‚   â”œâ”€â”€ hand_pose/                  # Hand pose factories and types
â”‚   â”œâ”€â”€ kinematics/                 # URDF / FK / IK wrappers (pinocchio optional)
â”‚   â””â”€â”€ tracking/                   # Tracking & Kalman smoothing
â”œâ”€â”€ scripts/                        # Utility scripts and wrappers (headless helpers)
â”œâ”€â”€ src/                            # Installable package entry points (alternate layout)
â”œâ”€â”€ tests/                          # Unit + integration tests (pytest)
â”œâ”€â”€ yolov8n.pt                      # Example pretrained weights (optional)
â”œâ”€â”€ main.py                         # Primary demo / entry script
â”œâ”€â”€ pick_and_place.py               # Pick-and-place example harness
â”œâ”€â”€ camera_setup.py                 # Camera helper utilities
â”œâ”€â”€ test_gripper_only.py            # Quick smoke test script
â”œâ”€â”€ environment.yml                 # Conda environment spec (recommended)
â”œâ”€â”€ requirements.txt                # pip requirements (lighter alternative)
â”œâ”€â”€ TROUBLESHOOTING.md              # Headless/runtime troubleshooting guide
â”œâ”€â”€ CODE_INSPECTION_REPORT.md       # Recent code inspection summary
â””â”€â”€ FIXES_APPLIED.md                # Summary of fixes applied in branch `jb_test`
```

## Installation

### Prerequisites (recommended)
- Python 3.8+ (3.10 used for development)
- Conda (recommended) or virtualenv
- Optional hardware: webcam/Arducam, SO-101 robot
- Optional GPU for fast YOLOv8 inference (CUDA-enabled)

Notes on optional dependencies:
- `pinocchio` (for advanced kinematics) is optional and only required if you use URDF/FK/IK features. The code lazy-loads `pinocchio` so core features work without it.
- YOLOv8 (`ultralytics`) is optional for object detection â€” you can use the system without it and add weights later.

### Recommended (Conda) install
1. Clone the repo and enter directory:
```bash
git clone https://github.com/ABMI-software/hand_controlLerobot.git
cd hand_controlLerobot
```

2. Create and activate the conda env:
```bash
conda env create -f environment.yml
conda activate hand_control
```

3. Install the package in editable mode:
```bash
pip install -e .
```

4. (Optional) Install `pinocchio` for kinematics (platform dependent):
Follow your platform's instructions; on many Linux systems a conda package is available.

### Pip / venv alternative
```bash
python -m venv hand_control
source hand_control/bin/activate
pip install -r requirements.txt
pip install -e .
```

### Verify installation
Run a quick smoke test (headless-friendly):
```bash
# in base terminal (with env active)
python -c "import sys,cv2; print('Python', sys.version.split()[0]); print('OpenCV ok', cv2.__version__)"
pytest -q tests/test_utils.py
```

If you have a camera attached, try `python main.py` (see Usage). If no camera is attached, the system will fall back to a synthetic camera thread; you can also pass a video file path to `--cam-idx` to emulate a camera.

## Usage

### Basic Operation

#### Hand Tracking Only (local GUI)
Start the demo with a camera (index, device path, or video file):
```bash
# camera index 0
python main.py --cam-idx 0

# or use a video file for deterministic tests
python main.py --cam-idx /path/to/test_video.mp4
```

Notes:
- `--cam-idx` accepts either an integer camera index, a device path (e.g. `/dev/video0`) or a video file path.
- If no camera is available the system will fall back to a synthetic camera thread for CI/headless runs.

#### Robot Control / Teleoperation
Run teleoperation using the provided demo scripts. Example (adjust port and options for your hardware):
```bash
# example with SO-101 enabled (adjust serial port)
python3 scripts/gripper_direct_jog.py --hand right --model wilor --cam-idx 0 \
    --so101-enable --so101-port /dev/serial/by-id/usb-XXXXX --verbose
```

You can also run older demo wrappers (e.g. `test_gripper_only.py`) for quick smoke tests.

#### Object Detection and Picking
Start the pick-and-place harness (object detector optional):
```bash
python pick_and_place.py --cam-idx 0
```

Controls (keyboard while running):
- `a` : switch to auto-pick mode
- `t` : switch to teleoperation mode
- `q` : quit

Modes supported:
- Direct Control: joint-level control via hand gestures
- Task Space: end-effector Cartesian control
- Gripper Control: pinch gestures to open/close gripper

The pick-and-place harness will use YOLOv8 for detection if weights and `ultralytics` are available; otherwise it runs in degraded mode.

### Headless / CI-friendly Usage
For headless systems (CI, servers, WSL) set the environment variables to avoid GUI backends:
```bash
export QT_QPA_PLATFORM=offscreen
export MPLBACKEND=Agg
# then run headless script
bash scripts/run_poke_motor.sh
```

`scripts/run_poke_motor.sh` configures a headless environment and runs the demo; use it when no display server is available.

### Advanced Features
- Gesture Recording: Save and replay common movement sequences
- Safety Limits: Built-in joint and velocity limits
- Multiple Tracking Models: Switch between different hand tracking models

## Configuration

### Tracking Settings
```bash
# Select tracking model (options: mediapipe, wilor)
python main.py --tracker mediapipe

# Adjust tracking sensitivity
python main.py --sensitivity 0.8
```

### Robot & Hardware Settings
- Joint speed limits and robot-specific params: `config/robot_config.yaml` (create if absent)
- Gesture mappings: `config/gesture_mapping.yaml`
- Camera calibration: `config/camera_config.yaml` or use OpenCV calibration utilities

### Camera & Device Notes
- `--cam-idx` accepts an integer camera index, a device path (e.g. `/dev/video0`), or a video file path.
- When no camera is present the system uses a synthetic camera fallback so tests and demos remain runnable in CI. For automated tests prefer passing a short video file to `--cam-idx`.

### Optional Dependencies (summary)
- `pinocchio`: kinematics â€” optional, lazy-loaded
- `ultralytics` / YOLOv8: object detection â€” optional, used by `pick_and_place.py`
- `lerobot`: robot communication â€” required for SO-101 control

Add or pin these in your environment as needed; see `environment.yml` for the recommended development stack.

## Troubleshooting

### Common Issues
1. **Robot Not Detected**
   - Check USB connection
   - Verify correct port permissions
   - Run `python scan_bus.py` to detect connected devices

2. **Poor Tracking Performance**
   - Ensure good lighting conditions
   - Check webcam resolution settings
   - Try different tracking models

3. **Unexpected Robot Movement**
   - Verify calibration settings
   - Check gesture sensitivity settings
   - Ensure clean background for better tracking

## Contributing
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ReadMe par Thomas, sur jetson orin nano
Lors de l'installation sur jetson, je conseille la mÃ©thode par venv. 
```bash
python -m venv hand_control
source hand_control/bin/activate
```
Il faut une version de python 3.10 (celle que j'utilise) ou 3.8. 
Certaines bibliothÃ¨ques demandent une installation manuelle comme chumpy : 
```bash
#installation manuelle de chumpy
pip install --no-build-isolation "chumpy @ git+https://github.com/mattloper/chumpy"

#installation des requirements sans les bibliothÃ¨ques non installables (comme wilor)
pip install --no-build-isolation -r requirements.txt
pip install -e .
```
Lors de l'utilisation, il faut utiliser le model mediapipe car Wilor ne fonctionne pas sur la jetson 
Pour un test de dÃ©tection de la main, il faut faire 
```bash
python main.py --model mediapipe --cam-idx 0
```
Pour contrÃ´ler le robot, j'ai modifiÃ© le code main.py pour envoyer les informations au Robot Moveo par node ros2. 
J'utilise un pc sous unbuntu pour la simulation gazebo, reliÃ© par un cÃ¢ble ethernet Ã  la jetson orin nano. 
Voici le protocole pour le faire fonctionner : 
Sur le PC (lancer la simulation) : 
```bash
cd ~/Robot5A-Simulation
source /opt/ros/humble/setup.bash
source install/setup.bash
ros2 launch robot_control visual_sim.launch.py

# Pour afficher les commandes envoyÃ© par la jetson : 
ros2 topic echo /arm_controller/joint_trajectory
```
Sur la Jetson (dÃ©tÃ©ction de la main et commande robot) :
```bash
cd ~/hand_controlLerobot
source hand_control/bin/activate
source /opt/ros/humble/setup.bash   

python main.py --model mediapipe --cam-idx 0 --so101-enable

```
Sur la jetson un cadre apparait, il faut appuyer sur la touche espace pour calibrer notre main au coordonnÃ©es centre. 
Les limites de coordonnÃ©es sont : 

0,13  < x < 0,36
-0,23 < y < 0,23
0,008 < z < 0,25

Les mouvements de la main vont contrÃ´ler le robot comme une manette : 

Axe 		articulation 		Type de rotation 
x		shoulder_lift		LÃ¨ve/abaisse lâ€™Ã©paule dans le plan sagittal
y		shoulder_pan		Rotation autour de lâ€™axe vertical (tourne lâ€™Ã©paule droite/gauche)
z		elbow_flex + wristflex 	Plie/dÃ©plie le coude + Plie/dÃ©plie le poignet

Pour envoyer des commandes manuelles au robot depuis la jetson, par exemple qu'il revienne Ã  une position initiale : 
```bash
ros2 topic pub /arm_controller/joint_trajectory trajectory_msgs/JointTrajectory "
joint_names: ['R0_Yaw','R1_Pitch','R2_Pitch','R3_Yaw','R4_Pitch']
points:
- positions: [0, 0, 0, 0, 0]
  velocities: []
  accelerations: []
  effort: []
  time_from_start:
    sec: 1
    nanosec: 0
    
"

# Et pour faire un mouvement pour la position [0.5, -0.3, 0.2, 0, 0] par exemple :
ros2 topic pub /arm_controller/joint_trajectory trajectory_msgs/JointTrajectory "
joint_names: ['R0_Yaw','R1_Pitch','R2_Pitch','R3_Yaw','R4_Pitch']
points:
- positions: [0.5, -0.3, 0.2, 0, 0]
  time_from_start: {sec: 2, nanosec: 0}

"
```
