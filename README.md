# Lerobot Controlled by hand

[![License](https://img.shields.io/github/license/ABMI-software/hand_controlLerobot)](LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.6%2B-blue)](https://www.python.org/downloads/)

## Overview
A robot arm teleoperation system based on hand tracking via webcam. Lerobot Control enables intuitive control of the SO-101 robot through natural hand gestures, bridging the gap between human motion and robotic control with a seamless real-time interface.

### Key Benefits
- ðŸŽ¯ Intuitive Control: Natural hand movements translate directly to robot actions
- âš¡ Real-time Response: Minimal latency between gesture recognition and robot movement
- ðŸ”„ Flexible Tracking: Multiple tracking models available for different use cases
- ðŸ›  Customizable: Adjustable sensitivity and control parameters

### What's New (Nov 17, 2025)
- **Headless & CI-friendly:** The repo now supports running on headless systems (WSL/CI) â€” see `scripts/run_poke_motor.sh` and the new `TROUBLESHOOTING.md` for env hints.
- **Clear camera handling:** Camera input accepts integer indices or file/device paths; code now fails with a clear, actionable message when no camera is present, and suggests using a video file for testing.
- **Lazy kinematics import:** The `pinocchio` dependency is loaded lazily â€” the app no longer fails at startup if `pinocchio` is missing (only required when using URDF/IK features).
- **Improved error messages & docs:** Helpful guidance added for common issues and step-by-step troubleshooting in `TROUBLESHOOTING.md`.
- **Unit tests:** New utility tests added for core mapping/clamping routines (`tests/test_utils.py`).


## Table of Contents
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)

## Features
- Real-time hand tracking and gesture recognition
- Control of robot joints and gripper
- Support for multiple tracking models (Wilor, MediaPipe)
- Object detection using YOLOv8 with Arducam
- Dual camera system (Arducam + webcam)
- Interactive mode selection (auto-pick/teleoperation)
- Adjustable parameters for sensitivity and control
- Safety limits and emergency stops
- Gesture recording and playback capabilities

## Project Structure
```
hand_control/
â”œâ”€â”€ hand_teleop/
â”‚   â”œâ”€â”€ cameras/                    # Camera handling
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ camera_manager.py      # Multi-camera management
â”‚   â”œâ”€â”€ detection/                  # Object detection
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ object_detector.py     # YOLOv8 implementation
â”‚   â”œâ”€â”€ gripper_pose/              # Gripper control
â”‚   â”œâ”€â”€ hand_pose/                 # Hand tracking
â”‚   â”œâ”€â”€ kinematics/                # Robot kinematics
â”‚   â””â”€â”€ tracking/                  # Motion tracking
â”œâ”€â”€ config/                        # Configuration files
â”œâ”€â”€ scripts/                       # Utility scripts
â””â”€â”€ assets/                        # Project assets
```

## Installation

### Prerequisites
- Python 3.6 or higher
- Conda (recommended for managing dependencies)
- Webcam with minimum 720p resolution
- Arducam camera for object detection
- SO-101 robot hardware setup
- USB connection to the robot
- CUDA-capable GPU (recommended for YOLOv8)

### Installation Methods

#### Using Conda (Recommended)
1. Clone the repository:
   ```bash
   git clone https://github.com/ABMI-software/hand_controlLerobot.git
   cd hand_control
   ```

2. Create and activate environment using provided file:
   ```bash
   conda env create -f environment.yml
   conda activate hand_control
   ```

3. Install the package in development mode:
   ```bash
   pip install -e .
   ```

#### Using pip
1. Clone the repository:
   ```bash
   git clone https://github.com/ABMI-software/hand_controlLerobot.git
   cd hand_control
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows use: venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   pip install -e .
   ```

#### Verify Installation
After installation, verify that everything is working:
```bash
python test_gripper_only.py
```

### Hardware Setup
1. Connect the SO-101 robot to your computer via USB
2. Ensure the webcam is properly connected and recognized
3. Position the webcam with a clear view of the control area

## Usage

### Basic Operation

#### Hand Tracking Only
1. Start camera - hand tracking and pose estimation:
   ```bash
   python main.py
   ```

#### Robot Control
2. Start teleoperation with lerobot and Arducam:
   ```bash
   python3 poke_motor.py --hand right --model wilor --cam-idx 0 --fps 30 \
          --so101-enable --so101-port /dev/serial/by-id/usb-1a86_USB_Single_Serial_5AA9018150-if00 \
          --invert-z --raw --raw-min 1700 --raw-max 3200 --verbose
   ```

3. Start teleoperation with lerobot and astra depth camera:
   ```bash
   python hand_teleop_local.py --hand right --model wilor --cam-idx -1 --fps 30 \
          --so101-enable --so101-port /dev/serial/by-id/usb-1a86_USB_Single_Serial_5AA9018150-if00 \
          --invert-z --raw --raw-min 1700 --raw-max 3200 --verbose --print-joints 
   ```

#### Object Detection and Picking
4. Start object detection and picking system:
   ```bash
   python pick_and_place.py
   ```

   Controls:
   - Press 'a' to switch to auto-pick mode
   - Press 't' to switch to teleoperation mode
   - Press 'q' to quit

4. Available control modes:
   - **Direct Control**: Control robot joints directly with hand movements
   - **Task Space**: Control end-effector position in Cartesian space
   - **Gripper Control**: Use pinch gesture to control gripper


5. to control wrist via mediapipe
    ```bash
    to be completed
    ```

### Advanced Features
- **Gesture Recording**: Save and replay common movement sequences
- **Safety Limits**: Built-in joint and velocity limits
- **Multiple Tracking Models**: Switch between different hand tracking models

## Configuration

### Tracking Settings
```bash
# Select tracking model (options: mediapipe, wilor)
python main.py --tracker mediapipe

# Adjust tracking sensitivity
python main.py --sensitivity 0.8
```

### Robot Settings
- Joint speed limits can be configured in `config/robot_config.yaml`
- Gesture mappings can be modified in `config/gesture_mapping.yaml`
- Camera calibration settings in `config/camera_config.yaml`

## Troubleshooting

### Common Issues
1. **Robot Not Detected**
   - Check USB connection
   - Verify correct port permissions
   - Run `python scan_bus.py` to detect connected devices

2. **Poor Tracking Performance**
   - Ensure good lighting conditions
   - Check webcam resolution settings
   - Try different tracking models

3. **Unexpected Robot Movement**
   - Verify calibration settings
   - Check gesture sensitivity settings
   - Ensure clean background for better tracking

## Contributing
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.